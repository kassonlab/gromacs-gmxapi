=========================
Work specification schema
=========================

..  contents::
    :local:

Changes in second version
=========================

- Use the term "work graph" instead of "work specification".
  ``gmx.workflow.WorkSpec`` is replaced with an interface for a view to a work graph owned
  by a Context instance.
- Schema version string changes from ``gmxapi_workspec_0_1`` to ``gmxapi_graph_0_2``
- ``gmx.workflow.WorkElement`` class is replaced with an interface definition
  for an instance of an Operation. Users no longer create objects representing
  work directly.
- Proposed: *``elements`` map name changed to ``nodes``?*
- Deprecate work graph operations ``gmxapi.md`` and ``gromacs.load_tpr``
- User-provided ``name`` properties are replaced with two new properties:
    - ``label`` optionally identifies the entity to the user.
    - ``uid`` is a unique identifier that is deterministically generated by the API to
      completely and verifiably characterize an entity in terms of its inputs to facilitate
      reproducibility, optimization, and flexibility in graph manipulation.
- Proposed: *Serialized work graphs may be bundled, nested as a map of named graphs. By
  convention, the graph named ``default`` would be examined as an entry point.*

Goals
=====

- Serializeable representation of a molecular simulation and analysis workflow.
- Simple enough to be robust to API updates and uncoupled from implementation details.
- Complete enough to unambiguously direct translation of work to API calls.
- Facilitate easy integration between independent but compatible implementation code in Python or C++
- Verifiable compatibility with a given API level.
- Provide enough information to uniquely identify the "state" of deterministic inputs and outputs.

The last point warrants further discussion.

One point to make is that we need to be able to recover the state of an
executing graph after an interruption, so we need to be able to identify whether or not work has been partially completed
and how checkpoint data matches up between nodes, which may not all (at least initially) be on the same computing host.

The other point that is not completely unrelated is how to minimize duplicated data or computation. Due to numerical
optimizations, molecular simulation results for the exact same inputs and parameters may not produce output that is
binary identical, but which should be treated as scientifically equivalent. We need to be able to identify equivalent
rather than identical output. Input that draws from the results of a previous operation should be able to verify whether
valid results for any identically specified operation exists, or at what state it is in progress.

The degree of granularity and room for optimization we pursue affects the amount of data in the work specification, its
human-readability / editability, and the amount of additional metadata that needs to be stored in association with a
Session.

If one element is added to the end of a work specification, results of the previous operations should not be invalidated.

If an element at the beginning of a work specification is added or altered, "downstream" data should be easily invalidated.

Serialization format
====================

The work specification record is valid JSON serialized data, restricted to the latin-1 character set, encoded in utf-8.

Uniqueness
----------

Goal: results should be clearly mappable to the work specification that led to them, such that the same work could be
repeated from scratch, interrupted, restarted, etcetera, in part or in whole, and verifiably produce the same results
(which can not be artificially attributed to a different work specification) without requiring recomputing intermediate
values that are available to the Context.

The entire record, as well as individual elements, have a well-defined hash that can be used to compare work for
functional equivalence.

State is not contained in the work specification, but state is attributable to a work specification.

If we can adequately normalize utf-8 Unicode string representation, we could checksum the full text, but this may be more
work than defining a scheme for hashing specific data or letting each operation define its own comparator.

Question: If an input value in a workflow is changed from a verifiably consistent result to an equivalent constant of a
different "type", do we invalidate or preserve the downstream output validity? E.g. the work spec changes from
"operationB.input = operationA.output" to "operationB.input = final_value(operationA)"

The question is moot if we either only consider final values for terminating execution or if we know exactly how many
iterations of sequenced output we will have, but that is not generally true.

Maybe we can leave the answer to this question unspecified for now and prepare for implementation in either case by
recording more disambiguating information in the work specification (such as checksum of locally available files) and
recording initial, ongoing, and final state very granularly in the session metadata. It could be that this would be
an optimization that is optionally implemented by the Context.

It may be that we allow the user to decide what makes data unique. This would need to be very clearly documented, but
it could be that provided parameters always become part of the unique ID and are always not-equal to unprovided/default
values. Example: a ``load_tpr`` operation with a ``checksum`` parameter refers to a specific file and immediately
produces a ``final`` output, but a ``load_tpr`` operation with a missing ``checksum`` parameter produces non-final
output from whatever file is resolved for the operation at run time.

It may also be that some data occurs as a "stream" that does not make an operation unique, such as log file output or
trajectory output that the user wants to accumulate regardless of the data flow scheme; or as a "result" that indicates
a clear state transition and marks specific, uniquely produced output, such as a regular sequence of 1000 trajectory
frames over 1ns, or a converged observable. "result"s must be mapped to the representation of the
workflow that produced them. To change a workflow without invalidating results might be possible with changes that do
not affect the part of the workflow that fed those results, such as a change that only occurs after a certain point in
trajectory time. Other than the intentional ambiguity that could be introduced with parameter semantics in the previous
paragraph,

Heuristics
----------

Dependency order affects order of instantiation and the direction of binding operations at session launch.

.. rubric:: Rules of thumb

An element can not depend on another element that is not in the work specification.
*Caveat: we probably need a special operation just to expose the results of a different work flow.*

Dependency direction affects sequencing of Director calls when launching a session, but also may be used at some point
to manage checkpoints or data flow state checks at a higher level than the execution graph.

Namespaces
==========

Python, work graph serialization spec, and extension modules
------------------------------------------------------------

I need to work on expressing it more clearly (maybe through Sphinx formatting),
but it is important to note that there are three different concepts implied by
the prefixes to names used here.

Names starting with ``gmx.`` are symbols in the Python ``gmx`` package.
Names starting with ``gmxapi.`` are not Python names, but work graph operations
defined for gmxapi and implemented by a gmxapi compatible execution Context.

Names starting with ``gromacs.`` are also work graph operations, but are implemented
throught GROMACS library bindings (currently ``gmx.core`` but it seems like we should
separate out).
They are less firmly specified because they
are dependent on GROMACS terminology, conventions, and evolution.
Operations
implemented by extension modules use a namespace equal to their importable module name.

The Context implementation in the Python package implements the runtime aspects
of gmxapi operations in submodules of ``gmx``, named (hopefully conveniently) the
same as the work graph operation or ``gmx`` helper function.

The procedural interface in the :py:mod:`gmx` module provides helper functions that produce handles to work graph
operations and to simplify more involved API tasks.

Operation and Object References
-------------------------------

Entities in a work graph also have (somewhat) human readable names with nested
scope indicated by ``.`` delimiters. Within the scope of a work node, namespaces
distinguish several types of interaction behavior. (See :ref:`grammar`.)
Within those scopes, operation definitions specify named "ports" that are
available for nodes of a given operation.
Port names and object types are defined in the API spec (for operations in the ``gmxapi``
namespace) and expressed through the lower level API.

The ports for a work graph node are accessible by proxy in the Python interface,
using correspondingly named nested attributes of a Python reference to the node.

Note: we need a unique identifier and a well defined scheme for generating them so
that the API can determine data flow, tag artifacts, and detect complet or partially
complete work. It could be that we should separate work node 'name' into 'uid'
and 'label', where 'label' is a non-canonical and non-required part of a work
graph representation.

Canonical work graph representation
===================================

*Define the deterministic way to identify a work graph and its artifacts for
persistence across interruptions and to avoid duplication of work...*

Core API roles and collaborations
=================================

Interfaces and/or Base classes
------------------------------

OperationFactory
~~~~~~~~~~~~~~~~

An OperationFactory receives a Context and Input, and returns an OperationHandle to the caller.

Context
~~~~~~~

Variations include

* GraphContext that just builds a graph that can be serialized for deserialization by another context.
* LaunchContext that processes a graph to be run in appropriate OperationContexts. Produces a Session.
* OperationContext or ImmediateContext that immediately executes the implemented operation

NodeBuilder
~~~~~~~~~~~

``addResource()`` Configures inputs, outputs, framework requirements, and factory functions.
``build()`` returns an OperationHandle

Operation
~~~~~~~~~

The OperationHandle returned by a factory may be a an implementing object or some sort of wrapper or proxy object.
output() provides getters for Results.

Has a Runner behavior.

Result
~~~~~~

gmxapi-typed output data. May be implemented with futures and/or proxies. Provides
extract() method to convert to a valid local owning data handle.

Behaviors
---------

Launcher
~~~~~~~~

Callable that accepts a Session (Context) and produces a Runnable (Operation).
A special case of OperationDirector.

Runner
~~~~~~

Takes input, runs, and returns a Runner that can be called in the same way.

run() -> Runner
run(Runner::Resources) -> Runner

OperationDirector
~~~~~~~~~~~~~~~~~

Takes Context and Input to produce a Runner.

Use a Context to get one or more NodeBuilders to configure the operation in a new context.
Then return a properly configured OperationHandle for the context.

Graph
~~~~~

Can produce NodeBuilders, store directorFactories.

Can serialize / deserialize the workflow representation.

* ``serialize()``
* ``uid()``
* ``newOperation()``: get a NodeBuilder
* ``node(uid)``: getter
* ``get_node_by_label(str)``: find uid
* iterator

OutputProxy
~~~~~~~~~~~

Service requests for Results for an Operator's output nodes.

Input
~~~~~

Input is highly dependent on the implementation of the operation and the context in which
it is executing. The important thing is that it is something that can be interpreted by a DirectorFactory.

Arbitrary lists of arguments and keyword arguments can be accepted by a Python
module director to direct the construction of one or more graph nodes or to
get an immediately executed runner.

GraphNode or serialized Operation Input is accepted by a LaunchContext or
DispatchingContext.

A runner implementing the operation execution accepts Input in the form of
SessionResources.

Middleware API
==============

The work graph has a basic grammar and structure that maps well to basic data structures.
We use JSON for serialization of a Python dictionary.

Integers and floating point numbers are 64-bit.

The JSON data should be utf-8 compatible, but note that JSON codecs probably map Unicode string
objects on the program side to un-annotated strings in the serialized data
(encoding is at the level of the entire byte stream).

.. _grammar:

Work graph grammar
------------------

Names (labels and UIDs) in the work graph are strings from the ASCII / Latin-1 character set.
Periods (``.``) have special meaning as delimiters.

Bare string values are interpreted as references to other work graph entities. Strings in
lists are interpreted as strings.

TODO: I am testing a possible convention for helper function, factory function, and implementation.
For the ``gmxapi`` work graph operation name space, ``gmx.context`` looks for
operations in ``gmx._<name>``, where *<name>* is the operation name. We will
probably adopt a slightly different convention for the ``gromacs`` name space.
For all other name spaces, the package attempts to import the namespace as a
module and to look for operation implementations as a module attribute matching
the name of the operation. The attribute may be a callable or an object implementing
the Operation interface. Such an object may itself be a module.
(Note this would also provide a gateway to mixing Python and C++ in plugins,
since the namespace module may contain other code or import additional modules.)

The work graph node ``output`` contains a ``file`` port that is
a gmxapiMap output of command line arguments to filenames.

.. versionadded:: 0.0.8

    Graph node structure::
    'elements': {
        'cli_op_aaaaaa': {
            #'label': 'exe1',
            'namespace': 'gmxapi',
            'operation': 'commandline',
            'params': {
                'executable': [],
                'arguments': [[]],
            },
            'depends': []
        },
        'cli_op_bbbbbb': {
            #'label': 'exe2',
            'namespace': 'gmxapi',
            'operation': 'commandline',
            'params': {
                'executable': [],
                'arguments': [[], []],
            },
            'depends': ['cli_op_aaaaaa']
        },
    }

.. versionchanged:: 0.1

    Output ports are determined by querying the operation.

    Graph node structure::
    'elements': {
        'filemap_aaaaaa': {
            operation: 'make_map',
            'input': {
                '-f': ['some_filename'],
                '-t': ['filename1', 'filename2']
            },
            # 'output': {
            #     'file': 'gmxapi.Map'
            # }
        },
        'cli_op_aaaaaa': {
            'label': 'exe1',
            'namespace': 'gmxapi',
            'operation': 'commandline',
            'input': {
                'executable': [],
                'arguments': [[]],
                # this indirection avoids naming complications. Alternatively,
                # we could make parsing recursive and allow arbitrary nesting
                # with special semantics for dictionaries (as well as lists)
                'input_file_arguments': 'filemap_aaaaaa',
            },
            # 'output': {
            #     'file': 'gmxapi.Map'
            # }
        },
        'filemap_bbbbbb: {
            'label': 'exe1_output_files',
            'namespace': 'gmxapi',
            'operation': 'make_map',
            'input': {
                '-in1': 'cli_op_aaaaaa.output.file.-o',
                '-in2': ['static_fileB'],
                '-in3': ['arrayfile1', 'arrayfile2'] # matches dimensionality of inputs
            }
        },
        'cli_op_bbbbbb': {
            'label': 'exe2',
            'namespace': 'gmxapi',
            'operation': 'commandline',
            'input': {
                'executable': [],
                'arguments': [],
                'input_file_arguments': 'filemap_bbbbbb'
            },
        },

    }

Operations
----------

Each node in a work graph represents an instance of an Operation.
The API specifies operations that a gmxapi-compliant execution context *should* provide in
the ``gmxapi`` namespace.

All specified ``gmxapi`` operations are provided by the reference implementation in released
versions of the ``gmx`` package. ``gmx.context.Context`` also provides operations in the ``gromacs``
namespace. This support will probably move to a separate module, but the ``gromacs`` namespace
is reserved and should not be reimplemented in external software.

When translating a work graph for execution, the Context calls a factory function for each
operation to get a Director. A Python-based Context *should* consult an internal map for
factory functions for the ``gmxapi`` namespace. **TODO:** *How to handle errors?
We don't really want middleware clients to have to import ``gmx``, but how would a Python
script know what exception to catch? Errors need to be part of an API-specified result type
or protocol, and/or the exceptions need to be defined in the package implementing the context.*


namespace is imported

operation is an attribute in namespace that

..  versionadded:: 0.0

    is callable with the signature ``operation(element)`` to get a Director

..  versionchanged:: 0.1

    has a ``_gmxapi_graph_director`` attribute to get a Director

Helper
~~~~~~

Add operation instance to work graph and return a proxy object.
If proxy object has ``input`` or ``output`` attributes, they should forward ``getattr``
calls to the context... *TBD*

The helper makes API calls to the default or provided Context and then asks the Context for
an object to return to the caller. Generally, this is a proxy Operation object, but when the
context is a local context in the process of launching a session, the object can be a
graph Director that can be used to finish configuring and launch the execution graph.

Signatures

``myplugin.myoperation(arg0: WorkElement) -> gmx.Operation``

..  versionchanged:: 0.1

    Operation helpers are no longer required to accept a ``gmx.workflow.WorkElement`` argument.

``myplugin.myoperation(*args, input: inputobject, output: outputobject, **kwargs)``

    inputobject : dict
        Map of named input ports to typed gmxapi data, implicitly mappable Python objects,
        or objects implementing the gmxapi Output interface.

Some operations (``gmx.commandline``) need to provide an ``output`` keyword argument to define
data types and/or placeholders (not represented in the work graph).
    outputobject : dict
        Map of named output ports to

Additional ``args`` and ``kwargs`` may be used by the helper function to set up the work
graph node. Note that the context will not use them when launching the operation, though,
so ....


... Maybe let ``input`` and ``output`` kwargs be interpreted by the helper function, too,
and let the operation node input be completely specified by ``parameters``?

``myplugin.myoperation(arg0: graph_ref, *args, parameters: inputobject, **kwargs)``

... I think we can go ahead and let ``gmx.Operation.input`` and ``gmx.Operation.output``
implement ``get_item``...

Implementation note: the input and output attributes can have common implementations,
provided with Python "Descriptor"s

Servicing the proxy
~~~~~~~~~~~~~~~~~~~

When the Python client added the operation to the work graph, it used a helper function
to get a reference to an Operation proxy object. This object holds a weak reference to
the context and work graph to which it was added.


Factory
~~~~~~~

get Director for session launch

Director
~~~~~~~~

subscribable to implement data dependencies

``build`` method adds ``launch`` and ``run`` objects to execution graph.

To do: change ``build`` to ``construct``

Session callable
~~~~~~~~~~~~~~~~

``gmxapi`` operations
---------------------

Operation namespace: gmxapi


.. rubric:: operation: make_input

.. versionadded:: gmxapi_graph_0_2

Produced by :py:func:`gmx.make_input`

* ``input`` ports

  - ``params``
  - ``structure``
  - ``topology``
  - ``state``

* ``output`` ports

  - ``params``
  - ``structure``
  - ``topology``
  - ``state``


.. rubric:: operation: md

.. versionadded:: gmxapi_workspec_0_1

.. deprecated:: gmxapi_graph_0_2

Produced by :py:func:`gmx.workflow.from_tpr`

Ports:

* ``params``
* ``depends``


.. rubric:: operation: modify_input

.. versionadded:: gmxapi_graph_0_2

Produced by :py:func:`gmx.modify_input`

* ``input`` ports

  - ``params``
  - ``structure``
  - ``topology``
  - ``state``

* ``output`` ports

  - ``params``
  - ``structure``
  - ``topology``
  - ``state``


``gromacs`` operations
----------------------

Operation namespace: gromacs


.. rubric:: operation: load_tpr

.. versionadded:: gmxapi_workspec_0_1

.. deprecated:: gmxapi_graph_0_2

Produced by :py:func:`gmx.workflow.from_tpr`


.. rubric:: operation: mdrun

.. versionadded:: gmxapi_graph_0_2

Produced by :py:func:`gmx.mdrun`

* ``input`` ports

  - ``params``
  - ``structure``
  - ``topology``
  - ``state``

* ``output`` ports

  - ``trajectory``
  - ``conformation``
  - ``state``

* ``interface`` ports

  - ``potential``


.. rubric:: operation: read_tpr

.. versionadded:: gmxapi_graph_0_2

Produced by :py:func:`gmx.read_tpr`

* ``input`` ports

  - ``params`` takes a list of filenames

* ``output`` ports

  - ``params``
  - ``structure``
  - ``topology``
  - ``state``


Extension API
=============

Extension modules provide a high-level interface to gmxapi operations with functions
that produce Operation objects. Operation objects maintain a weak reference to the
context and work graph to which they have been added so that they can provide a
consistent proxy interface to operation data. Several object properties provide
accessors that are forwarded to the context.

.. These may seem like redundant scoping while operation instances are essentially
   immutable, but with more graph manipulation functionality, we can make future
   operation proxies more mutable. Also, we might add extra utilities or protocols
   at some point, so we include the scoping from the beginning.

``input`` contains the input ports of the operation. Allows a typed graph edge. Can
contain static information or a reference to another gmxapi object in the work graph.

``output`` contains the output ports of the operation. Allows a typed graph edge. Can
contain static information or a reference to another gmxapi object in the work graph.

``interface`` allows operation objects to bind lower-level interfaces at run time.

Connections between ``input`` and ``output`` ports define graph edges that can be
checkpointed by the library with additional metadata.

Python interface
================


:py:func:`gmx.read_tpr` creates a node for a ``gromacs.read_tpr`` operation implemented
with :py:func:`gmx.fileio.read_tpr`

:py:func:`gmx.mdrun` creates a node for a ``gromacs.mdrun`` operation, implemented
with :py:func:`gmx.context._mdrun`

:py:func:`gmx.init_subgraph`

:py:func:`gmx.while_loop` creates a node for a `gmxapi.while_loop


Work graph procedural interface
-------------------------------

Python syntax available in the imported ``gmx`` module.

..  py:function:: gmx.commandline_operation(executable, arguments=[], input=[], output=[])

    .. versionadded:: 0.0.8

    lorem ipsum

..  py:function:: gmx.get_context(work=None)
    :noindex:

    .. versionadded:: 0.0.4

    Get a handle to an execution context that can be used to launch a session
    (for the given work graph, if provided).

..  py:function:: gmx.logical_not

    .. versionadded:: 0.1

    Create a work graph operation that negates a boolean input value on its
    output port.

..  py:function:: gmx.make_input()
    :noindex:

    .. versionadded:: 0.1

..  py:function:: gmx.mdrun()

    .. versionadded:: 0.0.8

    Creates a node for a ``gromacs.mdrun`` operation, implemented
    with :py:func:`gmx.context._mdrun`

..  py:function:: gmx.modify_input()

    .. versionadded:: 0.0.8

    Creates a node for a ``gmxapi.modify_input`` operation. Initial implementation
    uses ``gmx.fileio.read_tpr`` and ``gmx.fileio.write_tpr``

..  py:function:: gmx.read_tpr()

    .. versionadded:: 0.0.8

    Creates a node for a ``gromacs.read_tpr`` operation implemented
    with :py:func:`gmx.fileio.read_tpr`

..  py:function:: gmx.gather()

    .. versionadded:: 0.0.8

..  py:function:: gmx.reduce()

    .. versionadded:: 0.1

    Previously only available as an ensemble operation with implicit reducing
    mode of ``mean``.

..  py:function:: gmx.run(work=None, **kwargs)
    :noindex:

    Run the current work graph, or the work provided as an argument.

    .. versionchanged:: 0.0.8

    ``**kwargs`` are passed to the gmxapi execution context. Refer to the
    documentation for the Context for usage. (E.g. see :py:class:`gmx.context.Context`)

..  py:function:: gmx.init_subgraph()

    .. versionadded:: 0.1

    Prepare a subgraph. Alternative name: ``gmx.subgraph``

..  py:function:: gmx.tool

    .. versionadded:: 0.1

    Add a graph operation for one of the built-in tools, such as a GROMACS
    analysis tool that would typically be invoked with a ``gmx toolname <args>``
    command line syntax. Improves interoperability of tools previously accessible
    only through :py:func:`gmx.commandline_operation`

..  py:function:: gmx.while_loop()

    .. versionadded:: 0.1

    Creates a node for a ``gmxapi.while_loop``

Types
-----

Python classes for gmxapi object and data types.

..  py:class:: gmx.InputFile

    Proxy for

..  py:class:: gmx.NDArray

    N-dimensional array of gmxapi data.

..  py:class:: gmx.OutputFile

    Proxy for

Additional classes and specified interfaces
-------------------------------------------

We support Python duck-typing where possible, in that objects do not need to
inherit from a gmxapi-provided base class to be compatible with specified
gmxapi behaviors. This section describes the important attributes of specified
gmxapi interfaces.

This section also notes

* classes in the reference implementation that implement specified interfaces
* utilities and helpers provided to support creating gmxapi compatible wrappers

.. rubric:: Operation

Utilities
---------

..  py:function:: gmx.operation.make_operation(class, input=[], output=[])

    Generate a function object that can be used to manipulate the work graph
    _and_ to launch the custom-defined work graph operation.

    Example: https://github.com/kassonlab/gmxapi-scripts/blob/master/analysis.py

Reference implementation
------------------------

The ``gmx`` Python package implements ``gmxapi`` operations in the ``gmx.context.Context``
reference implementation to support top-level ``gmx`` functions using various
``gmx`` submodules.

:py:func:`gmx.fileio.read_tpr` Implements :py:func:`gromacs.read_tpr`

Specification module
====================

Documentation for the specification should be extracted from the package.
It will be migrated to :module:`gmx._workspec_0_2`.

The module (eventually) provides helpers, with utilities to validate API implementations / data structures / interfaces,
and (possibly) wrappers or factories.

The remaining content in this document is automatically extracted from the
:py:mod:`gmx._workspec_0_2` module. The above content can be migrated into this
module shortly, but the intent is that the module will also contain syntax and
schema checkers.

Specification
-------------

..  automodule:: gmx._workspec_0_2
    :members:

Helpers
-------

..  automodule:: gmx._workspec_0_2.util
    :members:
